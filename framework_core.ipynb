{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# CyberBrain: Cybersecurity in BCI for Advanced Driver Assistance\n",
    "## Milestone MS3: Framework to detect and measure the cyberattacks impact.\n",
    "#### University of Murcia, Spain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: https://univmurcia-my.sharepoint.com/:u:/g/personal/enriquetomas_um_es/EUqf9NlxBC9HudvMJ4FUvJMBcU4ngGDug5bobNka_p9FwQ?e=Rx1lc3\r\n"
     ]
    }
   ],
   "source": [
    "!wget -O dataset/p300-umu https://univmurcia-my.sharepoint.com/:u:/g/personal/enriquetomas_um_es/EUqf9NlxBC9HudvMJ4FUvJMBcU4ngGDug5bobNka_p9FwQ?e=Rx1lc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset=\"dataset/p300-umu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "with open(dataset, 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from framework_acquisition import *\n",
    "t = threading.Thread(name='framework_acquisition', target=acquire_signals(), args=(data,))\n",
    "t.start()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "subjects = {}\n",
    "for i, d in enumerate(data):\n",
    "    subjects[f'Subject {i}'] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'Subject 0': <EpochsArray |  3982 events (all good), 0 - 0.585938 sec, baseline off, ~37.0 MB, data loaded,\n  'neg': 3413\n  'pos': 569>,\n 'Subject 1': <EpochsArray |  3916 events (all good), 0 - 0.585938 sec, baseline off, ~36.4 MB, data loaded,\n  'neg': 3362\n  'pos': 554>,\n 'Subject 2': <EpochsArray |  2053 events (all good), 0 - 0.585938 sec, baseline off, ~19.1 MB, data loaded,\n  'neg': 1760\n  'pos': 293>,\n 'Subject 3': <EpochsArray |  6516 events (all good), 0 - 0.585938 sec, baseline off, ~60.5 MB, data loaded,\n  'neg': 5589\n  'pos': 927>,\n 'Subject 4': <EpochsArray |  3396 events (all good), 0 - 0.585938 sec, baseline off, ~31.5 MB, data loaded,\n  'neg': 2912\n  'pos': 484>,\n 'Subject 5': <EpochsArray |  3975 events (all good), 0 - 0.585938 sec, baseline off, ~36.9 MB, data loaded,\n  'neg': 3404\n  'pos': 571>,\n 'Subject 6': <EpochsArray |  1163 events (all good), 0 - 0.585938 sec, baseline off, ~10.8 MB, data loaded,\n  'neg': 871\n  'pos': 292>,\n 'Subject 7': <EpochsArray |  1174 events (all good), 0 - 0.585938 sec, baseline off, ~10.9 MB, data loaded,\n  'neg': 1006\n  'pos': 168>,\n 'Subject 8': <EpochsArray |  4139 events (all good), 0 - 0.585938 sec, baseline off, ~38.4 MB, data loaded,\n  'neg': 3549\n  'pos': 590>,\n 'Subject 9': <EpochsArray |  4047 events (all good), 0 - 0.585938 sec, baseline off, ~37.6 MB, data loaded,\n  'neg': 3466\n  'pos': 581>,\n 'Subject 10': <EpochsArray |  4060 events (all good), 0 - 0.585938 sec, baseline off, ~37.7 MB, data loaded,\n  'neg': 3479\n  'pos': 581>,\n 'Subject 11': <EpochsArray |  4067 events (all good), 0 - 0.585938 sec, baseline off, ~37.8 MB, data loaded,\n  'neg': 3487\n  'pos': 580>,\n 'Subject 12': <EpochsArray |  4083 events (all good), 0 - 0.585938 sec, baseline off, ~37.9 MB, data loaded,\n  'neg': 3503\n  'pos': 580>,\n 'Subject 13': <EpochsArray |  4024 events (all good), 0 - 0.585938 sec, baseline off, ~37.4 MB, data loaded,\n  'neg': 3449\n  'pos': 575>,\n 'Subject 14': <EpochsArray |  2263 events (all good), 0 - 0.585938 sec, baseline off, ~21.0 MB, data loaded,\n  'neg': 1953\n  'pos': 310>,\n 'Subject 15': <EpochsArray |  3809 events (all good), 0 - 0.585938 sec, baseline off, ~35.4 MB, data loaded,\n  'neg': 3259\n  'pos': 550>}"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "target = subjects['Subject 1']['pos']\n",
    "nonTarget = subjects['Subject 1']['neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "target_data = subjects['Subject 1']['pos'].get_data()\n",
    "nontarget_data = subjects['Subject 1']['neg'].get_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target and non-target labels are balanced for correct classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "(554, 16, 76)"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "(3362, 16, 76)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nontarget_data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "alldata = X = np.concatenate([target_data, nontarget_data])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "(3916, 16, 76)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldata.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nontarget_data = nontarget_data[:593][:][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X, Y = [], []\n",
    "\n",
    "X = np.concatenate([target_data, nontarget_data])\n",
    "Y = np.concatenate([np.ones(target_data.shape[0]), np.zeros(nontarget_data.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(1147,)"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "(1147, 16, 76)"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data classification use case 1: P300 detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we are going to perform a test of how use case 1 should work for P300 detection. For this, we have created a simple binary classifier that allows the detection of P300."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with Logistic regression algorithm and cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NoTarget       0.78      0.77      0.78       593\n",
      "      Target       0.76      0.76      0.76       554\n",
      "\n",
      "    accuracy                           0.77      1147\n",
      "   macro avg       0.77      0.77      0.77      1147\n",
      "weighted avg       0.77      0.77      0.77      1147\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from mne.decoding import Vectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Instancia validador cruzado\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Instancia clasificador\n",
    "clf = make_pipeline(Vectorizer(), StandardScaler(), LogisticRegression(solver='liblinear', C=1, class_weight=\"balanced\"))\n",
    "\n",
    "# Proceso de validación cruzada\n",
    "preds = np.empty(len(Y))\n",
    "for train, test in cv.split(X, Y):\n",
    "    clf.fit(X[train], Y[train]) # ajustar\n",
    "    preds[test] = clf.predict(X[test]) #\n",
    "\n",
    "# Info del proceso\n",
    "target_names = ['NoTarget', 'Target']\n",
    "report = classification_report(Y, preds, target_names=target_names)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with Support Vector Machine algorithm and cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "param_grid = {'C': [0.1,1,10], 'gamma': [0.1,0.01],'kernel': ['rbf', 'linear']}\n",
    "#clf = SVC(kernel='linear', C=1, class_weight = \"balanced\")\n",
    "svc = SVC()\n",
    "clf = GridSearchCV(svc, param_grid)\n",
    "\n",
    "nsamples, nx, ny = X.shape\n",
    "d2_train_dataset = X.reshape((nsamples,nx*ny))\n",
    "\n",
    "# Proceso de validación cruzada\n",
    "preds = np.empty(len(Y))\n",
    "for train, test in cv.split(d2_train_dataset, Y):\n",
    "    clf.fit(d2_train_dataset[train], Y[train]) # ajustar\n",
    "    preds[test] = clf.predict(d2_train_dataset[test]) #\n",
    "\n",
    "# Info del proceso\n",
    "target_names = ['NoTarget', 'Target']\n",
    "report = classification_report(Y, preds, target_names=target_names)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with Random Forest algorithm and cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NoTarget       0.69      0.81      0.74       593\n",
      "      Target       0.75      0.60      0.67       554\n",
      "\n",
      "    accuracy                           0.71      1147\n",
      "   macro avg       0.72      0.71      0.71      1147\n",
      "weighted avg       0.72      0.71      0.71      1147\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "\n",
    "nsamples, nx, ny = X.shape\n",
    "d2_train_dataset = X.reshape((nsamples,nx*ny))\n",
    "\n",
    "# Proceso de validación cruzada\n",
    "preds = np.empty(len(Y))\n",
    "for train, test in cv.split(d2_train_dataset, Y):\n",
    "    clf.fit(d2_train_dataset[train], Y[train]) # ajustar\n",
    "    preds[test] = clf.predict(d2_train_dataset[test]) #\n",
    "\n",
    "# Info del proceso\n",
    "target_names = ['NoTarget', 'Target']\n",
    "report = classification_report(Y, preds, target_names=target_names)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the algorithms yield about 75% accuracy in classifying the P300. This could be improved by adjusting their hyperparameters, although this is not the objective. Noise is then applied to the signal until the previously trained classifier is unable to recognize the different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NoTarget       0.78      0.78      0.78        59\n",
      "      Target       0.76      0.76      0.76        55\n",
      "\n",
      "    accuracy                           0.77       114\n",
      "   macro avg       0.77      0.77      0.77       114\n",
      "weighted avg       0.77      0.77      0.77       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Maximum noise = 0.1\n",
    "max_noise = 0.1 # \n",
    "# Parameterized samples of a normal (Gaussian) distribution are created to generate noise in the signal.\n",
    "noise = np.random.normal(0.0, max_noise, X[test].shape)\n",
    "\n",
    "X_test_noise = X[test] + noise\n",
    "\n",
    "preds = clf.predict(X_test_noise) #\n",
    "\n",
    "# Info del proceso\n",
    "target_names = ['NoTarget', 'Target']\n",
    "report = classification_report(Y[test], preds, target_names=target_names)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NoTarget       0.55      0.58      0.56        59\n",
      "      Target       0.52      0.49      0.50        55\n",
      "\n",
      "    accuracy                           0.54       114\n",
      "   macro avg       0.53      0.53      0.53       114\n",
      "weighted avg       0.53      0.54      0.53       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Maximum noise = 1\n",
    "max_noise = 0.8\n",
    "noise = np.random.normal(0.0, max_noise, X[test].shape)\n",
    "X_test_noise = X[test] + noise\n",
    "\n",
    "preds = clf.predict(X_test_noise) #\n",
    "\n",
    "# Info del proceso\n",
    "target_names = ['NoTarget', 'Target']\n",
    "report = classification_report(Y[test], preds, target_names=target_names)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that with a maximum noise equal to 0.1, the classifier works correctly. However, when this noise is multiplied by 8, the classifier is unable to recognize the classes, therefore, this is our maximum noise allowed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create an unsupervised classification model to detect this noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset in 50% clean and 50% with noise\n",
    "X_clean, X_noise = train_test_split(X, test_size=0.50, random_state=42,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clean_train, X_clean_test = train_test_split(X_clean, test_size=0.10, random_state=42,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "contamination_factor = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model based on the IForest algorithm is trained with noise-free data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IForest(behaviour='old', bootstrap=False, contamination=0.05,\n",
       "    max_features=1.0, max_samples='auto', n_estimators=100, n_jobs=1,\n",
       "    random_state=42, verbose=0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyod.models.iforest import IForest\n",
    "\n",
    "clf = IForest(random_state=42, contamination=contamination_factor)\n",
    "\n",
    "nsamples, nx, ny = X_clean_train.shape\n",
    "d2_train_clean_dataset = X_clean_train.reshape((nsamples,nx*ny))\n",
    "\n",
    "nsamples, nx, ny = X_clean_test.shape\n",
    "d2_test_clean_dataset = X_clean_test.reshape((nsamples,nx*ny))\n",
    "# Model training\n",
    "clf.fit(d2_train_clean_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model based on the One-Class Support Vector Machine algorithm is trained with noise-free data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OCSVM(cache_size=200, coef0=0.0, contamination=0.05, degree=3, gamma=1e-05,\n",
       "   kernel='rbf', max_iter=-1, nu=0.5, shrinking=True, tol=0.001,\n",
       "   verbose=False)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyod.models.ocsvm import OCSVM\n",
    "\n",
    "clfOCSV = OCSVM(kernel='rbf',gamma=0.00001, contamination=contamination_factor)\n",
    "\n",
    "nsamples, nx, ny = X_clean_train.shape\n",
    "d2_train_clean_dataset = X_clean_train.reshape((nsamples,nx*ny))\n",
    "\n",
    "nsamples, nx, ny = X_clean_test.shape\n",
    "d2_test_clean_dataset = X_clean_test.reshape((nsamples,nx*ny))\n",
    "# Model training\n",
    "clfOCSV.fit(d2_train_clean_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier results are obtained with noise-free data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [0 1]      [50  8]\n"
     ]
    }
   ],
   "source": [
    "pred = clf.predict(d2_test_clean_dataset)\n",
    "unique_elements, counts_elements = np.unique(pred, return_counts=True)\n",
    "print(\"\\t\",unique_elements,\"    \",counts_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [0 1]      [46 12]\n"
     ]
    }
   ],
   "source": [
    "pred = clfOCSV.predict(d2_test_clean_dataset)\n",
    "unique_elements, counts_elements = np.unique(pred, return_counts=True)\n",
    "print(\"\\t\",unique_elements,\"    \",counts_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen how it identifies two different classes, this may be due to the differences in a signal in the p300 and non p300 stages. Next, we will add noise to the signal to see how the classifier behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_noise_train, X_noise_test = train_test_split(X_noise, test_size=0.10, random_state=42,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_noise = 0.8\n",
    "noise = np.random.normal(0.0, max_noise, X_noise_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_noise_test = X_noise_test + noise\n",
    "\n",
    "nsamples, nx, ny = X_noise_test.shape\n",
    "X_noise_new = X_noise_test.reshape((nsamples,nx*ny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [1]      [58]\n"
     ]
    }
   ],
   "source": [
    "pred = clf.predict(X_noise_new)\n",
    "unique_elements, counts_elements = np.unique(pred, return_counts=True)\n",
    "print(\"\\t\",unique_elements,\"    \",counts_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen how it identifies a single class related to the data with noise, which means that the classifier is working correctly. Next, we perform the last test in which data without noise and data with noise are combined in equal parts to test the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test_concatenado = np.concatenate([X_clean_test, X_noise_test])\n",
    "\n",
    "nsamples, nx, ny = dataset_test_concatenado.shape\n",
    "X_test_comb = dataset_test_concatenado.reshape((nsamples,nx*ny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [0 1]      [50 66]\n"
     ]
    }
   ],
   "source": [
    "pred = clf.predict(X_test_comb)\n",
    "unique_elements, counts_elements = np.unique(pred, return_counts=True)\n",
    "print(\"\\t\",unique_elements,\"    \",counts_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [0 1]      [47 69]\n"
     ]
    }
   ],
   "source": [
    "pred = clfOCSV.predict(X_test_comb)\n",
    "unique_elements, counts_elements = np.unique(pred, return_counts=True)\n",
    "print(\"\\t\",unique_elements,\"    \",counts_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the results are very close to 50% identification for each class, so the results are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will perform a series of possible attacks for the case in which the attacker has more information about the use case and how the data is being treated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate noise with random value whose maximum is the previously defined threshold (0.8). It will be applied intermittently in order to affect stages with P300 and not P300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = [], []\n",
    "\n",
    "X = np.concatenate([target_data, nontarget_data])\n",
    "Y = np.concatenate([np.zeros(X.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = 0\n",
    "X_mix = []\n",
    "Y_mix =[]\n",
    "for i in X:\n",
    "    if(cont%2 == 0):\n",
    "        noise = np.random.normal(0.0, np.random.uniform(0, 0.8), i.shape)\n",
    "        X_mix.append(i+noise)\n",
    "        Y_mix.append(1)\n",
    "    else:\n",
    "        X_mix.append(i)\n",
    "        Y_mix.append(0)\n",
    "    cont+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mix_train, X_mix_test = train_test_split(X_mix, test_size=0.20, random_state=42,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mix_test = np.array(X_mix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [0 1]      [161  69]\n"
     ]
    }
   ],
   "source": [
    "nsamples, nx, ny = X_mix_test.shape\n",
    "d2_test_mix_dataset = X_mix_test.reshape((nsamples,nx*ny))\n",
    "\n",
    "pred=clf.predict(d2_test_mix_dataset)\n",
    "unique_elements, counts_elements = np.unique(pred, return_counts=True)\n",
    "print(\"\\t\",unique_elements,\"    \",counts_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [0 1]      [221   9]\n"
     ]
    }
   ],
   "source": [
    "pred = clfOCSV.predict(d2_test_mix_dataset)\n",
    "unique_elements, counts_elements = np.unique(pred, return_counts=True)\n",
    "print(\"\\t\",unique_elements,\"    \",counts_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprobación con poco ruido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = [], []\n",
    "\n",
    "X = np.concatenate([target_data, nontarget_data])\n",
    "Y = np.concatenate([np.zeros(X.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = 0\n",
    "X_mix = []\n",
    "Y_mix =[]\n",
    "for i in X:\n",
    "    if(cont%2 == 0):\n",
    "        noise = np.random.normal(0.0, np.random.uniform(0, 0.1), i.shape)\n",
    "        X_mix.append(i+noise)\n",
    "        Y_mix.append(1)\n",
    "    else:\n",
    "        X_mix.append(i)\n",
    "        Y_mix.append(0)\n",
    "    cont+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mix_train, X_mix_test = train_test_split(X_mix, test_size=0.20, random_state=42,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mix_test = np.array(X_mix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [0 1]      [223   7]\n"
     ]
    }
   ],
   "source": [
    "nsamples, nx, ny = X_mix_test.shape\n",
    "d2_test_mix_dataset = X_mix_test.reshape((nsamples,nx*ny))\n",
    "\n",
    "pred=clf.predict(d2_test_mix_dataset)\n",
    "unique_elements, counts_elements = np.unique(pred, return_counts=True)\n",
    "print(\"\\t\",unique_elements,\"    \",counts_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [0 1]      [221   9]\n"
     ]
    }
   ],
   "source": [
    "pred = clfOCSV.predict(d2_test_mix_dataset)\n",
    "unique_elements, counts_elements = np.unique(pred, return_counts=True)\n",
    "print(\"\\t\",unique_elements,\"    \",counts_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcionalidad extra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insertamos ruido solo a target ****************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (554,16,76) (58,16,76) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "\u001b[1;32mC:\\Users\\MARIOQ~1\\AppData\\Local\\Temp/ipykernel_18560/621502659.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[0;32m      1\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget_data\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mnoise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnontarget_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnontarget_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (554,16,76) (58,16,76) "
     ]
    }
   ],
   "source": [
    "X, Y = [], []\n",
    "\n",
    "X = np.concatenate([target_data+noise, nontarget_data])\n",
    "Y = np.concatenate([np.ones(target_data.shape[0]), np.zeros(nontarget_data.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     NoRuido       0.69      0.81      0.74       593\n",
      "       Ruido       0.75      0.60      0.67       554\n",
      "\n",
      "    accuracy                           0.71      1147\n",
      "   macro avg       0.72      0.71      0.71      1147\n",
      "weighted avg       0.72      0.71      0.71      1147\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Instancia validador cruzado\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "nsamples, nx, ny = X.shape\n",
    "d2_train_dataset = X.reshape((nsamples,nx*ny))\n",
    "\n",
    "# Instancia clasificador\n",
    "clf = make_pipeline(StandardScaler(),RandomForestClassifier(max_depth=2, random_state=0))\n",
    "\n",
    "# Proceso de validación cruzada\n",
    "preds = np.empty(len(Y))\n",
    "for train, test in cv.split(d2_train_dataset, Y):\n",
    "    clf.fit(d2_train_dataset[train], Y[train]) # ajustar\n",
    "    preds[test] = clf.predict(d2_train_dataset[test]) #\n",
    "\n",
    "# Info del proceso\n",
    "target_names = ['NoRuido', 'Ruido']\n",
    "report = classification_report(Y, preds, target_names=target_names)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_noise = 0.000000001 # imperceptible a nivel de gráfica\n",
    "max_noise = 1 # perceptible a nivel de gráfica\n",
    "# Se crean muestras parametrizadas de una distribución normal (gaussiana) para generar ruido en la señal\n",
    "noise = np.random.normal(0.0, max_noise, nontarget_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probamos con non-target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = [], []\n",
    "\n",
    "X = np.concatenate([target_data, nontarget_data+noise])\n",
    "Y = np.concatenate([np.ones(target_data.shape[0]), np.zeros(nontarget_data.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     NoRuido       0.99      0.99      0.99       593\n",
      "       Ruido       0.99      0.99      0.99       554\n",
      "\n",
      "    accuracy                           0.99      1147\n",
      "   macro avg       0.99      0.99      0.99      1147\n",
      "weighted avg       0.99      0.99      0.99      1147\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instancia validador cruzado\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "nsamples, nx, ny = X.shape\n",
    "d2_train_dataset = X.reshape((nsamples,nx*ny))\n",
    "\n",
    "# Instancia clasificador\n",
    "clf = make_pipeline(StandardScaler(),RandomForestClassifier(max_depth=2, random_state=0))\n",
    "\n",
    "# Proceso de validación cruzada\n",
    "preds = np.empty(len(Y))\n",
    "for train, test in cv.split(d2_train_dataset, Y):\n",
    "    clf.fit(d2_train_dataset[train], Y[train]) # ajustar\n",
    "    preds[test] = clf.predict(d2_train_dataset[test]) #\n",
    "\n",
    "# Info del proceso\n",
    "target_names = ['NoRuido', 'Ruido']\n",
    "report = classification_report(Y, preds, target_names=target_names)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = [], []\n",
    "\n",
    "X = np.concatenate([target_data, nontarget_data])\n",
    "Y = np.concatenate([np.zeros(X.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1147, 16, 76)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1147,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}